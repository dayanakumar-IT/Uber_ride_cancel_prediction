# -*- coding: utf-8 -*-
"""MLB_G02_UBER_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ove0GJA18TNEdUupeKPzDtUU4GhWBE8l

# **Topic: Predicting Uber Ride Cancellations**

This project, Predicting Uber Ride Cancellations, uses a Kaggle dataset of ~148k rides from India/NCR to uncover why trips get cancelled. By cleaning data, exploring patterns, and building machine learning models, we aim to spot key factors behind cancellations and predict them before they happen.

> *The insights gained can help optimize platform operations, enhance rider experience, and support smarter decision-making for both business and customers.*

##**Data**


---


The dataset is taken from Kaggle and contains ~148,000 Uber rides from the India/NCR region. Dataset Link: https://www.kaggle.com/datasets/yashdevladdha/uber-ride-analytics-dashboard/data?select=ncr_ride_bookings.csv

##**Evaluation**


---


We will evaluate multiple machine learning models (Logistic Regression, Random Forest, XGBoost, CatBoost) using metrics like ROC-AUC, F1-score, and precision-recall balance. Our goal is to build a model that can achieve strong predictive performance, ideally exceeding 75% accuracy and robust recall for cancellations

| **Field**                             | **Description**                                                                 |
| ------------------------------------- | ------------------------------------------------------------------------------- |
| **Date**                              | Date of the booking                                                             |
| **Time**                              | Time of the booking                                                             |
| **Booking ID**                        | Unique identifier for each ride booking                                         |
| **Booking Status**                    | Status of booking (Completed, Cancelled by Customer, Cancelled by Driver, etc.) |
| **Customer ID**                       | Unique identifier for customers                                                 |
| **Vehicle Type**                      | Type of vehicle (Go Mini, Go Sedan, Auto, eBike/Bike, UberXL, Premier Sedan)    |
| **Pickup Location**                   | Starting location of the ride                                                   |
| **Drop Location**                     | Destination location of the ride                                                |
| **Avg VTAT**                          | Average time for driver to reach pickup location (in minutes)                   |
| **Avg CTAT**                          | Average trip duration from pickup to destination (in minutes)                   |
| **Cancelled Rides by Customer**       | Flag indicating customer-initiated cancellation                                 |
| **Reason for Cancelling by Customer** | Reason provided by customer for cancellation                                    |
| **Cancelled Rides by Driver**         | Flag indicating driver-initiated cancellation                                   |
| **Driver Cancellation Reason**        | Reason provided by driver for cancellation                                      |
| **Incomplete Rides**                  | Flag indicating incomplete ride                                                 |
| **Incomplete Rides Reason**           | Reason for incomplete rides                                                     |
| **Booking Value**                     | Total fare amount for the ride                                                  |
| **Ride Distance**                     | Distance covered during the ride (in km)                                        |
| **Driver Ratings**                    | Rating given to driver (1–5 scale)                                              |
| **Customer Rating**                   | Rating given by customer (1–5 scale)                                            |
| **Payment Method**                    | Method used for payment (UPI, Cash, Credit Card, Uber Wallet, Debit Card)       |

###Load the Dataset
"""

# Load
from google.colab import files
uploaded = files.upload()

"""### Downcasting for Effecient Memory Usage + Display Head"""

# Downcast
import pandas as pd
import numpy as np

def mb(nbytes: int) -> float:
    return round(nbytes / (1024**2), 2)

def downcast_uber_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Columns to protect
    protect_text = {
        'Booking ID', 'Customer ID',
        'Reason for cancelling by Customer',
        'Driver Cancellation Reason',
        'Incomplete Rides Reason'
    }
    protect_text = [c for c in protect_text if c in df.columns]

    before = df.memory_usage(deep=True).sum()

    for c in df.columns:
        if c in protect_text:
            continue

        s = df[c]

        # Downcast numeric columns only
        if pd.api.types.is_float_dtype(s):
            # Float
            df[c] = pd.to_numeric(s, downcast='float')

        elif pd.api.types.is_integer_dtype(s):
            # Integers
            df[c] = pd.to_numeric(s, downcast='integer')


    after = df.memory_usage(deep=True).sum()
    print(f"Downcast memory: {mb(before)} MB -> {mb(after)} MB (saved {mb(before-after)} MB)")
    return df


df = pd.read_csv('ncr_ride_bookings.csv')
df = downcast_uber_df(df)
display(df.dtypes.head(30))

"""### Checking Data types and Summary"""

import pandas as pd
from IPython.display import display

def missing_table(df: pd.DataFrame) -> pd.DataFrame:
    m = df.isna().sum()
    pct = (m / len(df) * 100).round(2)
    out = (
        pd.DataFrame({"missing": m, "missing_%": pct})
        .query("missing > 0")
        .sort_values("missing_%", ascending=False)
        .astype({"missing": "int"})
    )
    return out

def numeric_profile(df: pd.DataFrame) -> pd.DataFrame:
    num = df.select_dtypes(include="number")
    if num.empty:
        return pd.DataFrame()
    prof = num.describe().T
    prof["zeros_%"] = (num.eq(0).sum() / len(df) * 100).round(2)
    prof["negatives_%"] = (num.lt(0).sum() / len(df) * 100).round(2)
    return prof

def categorical_profile(df: pd.DataFrame) -> pd.DataFrame:
    cat = df.select_dtypes(exclude="number")
    if cat.empty:
        return pd.DataFrame()
    desc = cat.describe().T
    desc["unique_ratio_%"] = (desc["unique"] / len(df) * 100).round(2)
    return desc[["count", "unique", "unique_ratio_%", "top", "freq"]]

def constant_columns(df: pd.DataFrame):
    const = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]
    return const


print("Shape:", df.shape)
print("\nColumn dtypes & memory:")
df.info(memory_usage="deep")

print("\nMissing values (sorted):")
mt = missing_table(df)
display(mt if not mt.empty else pd.DataFrame({"missing": [], "missing_%": []}))

print("\nNumeric profile:")
display(numeric_profile(df))

print("\nCategorical profile:")
display(categorical_profile(df))

# Duplicates
print("\nDuplicate rows:", int(df.duplicated().sum()))
if "Booking ID" in df.columns:
    dup_keys = df["Booking ID"].duplicated().sum()
    print("Duplicate Booking IDs:", int(dup_keys))


const = constant_columns(df)
print("Constant columns:", const if const else "None")

"""## We have 1244 identified Duplicate  Booking IDs therefore we have decided to keep only one record per booking"""

# Which IDs repeat and how many times?
dup_counts = (df['Booking ID']
              .value_counts()
              .loc[lambda s: s > 1]
              .sort_values(ascending=False))
print("Duplicated IDs:", dup_counts.shape[0])
display(dup_counts.head(10))

# only few
sample_ids = dup_counts.index[:5]
display(df[df['Booking ID'].isin(sample_ids)]
        .sort_values(['Booking ID','Date','Time'])
        .head(50))

"""### Removing Duplicates"""

# Parse datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Time'] = pd.to_datetime(df['Time'], errors='coerce').dt.time

# Build combined timestamp
df['_ts'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')
pre_n = len(df)
dupe_mask = df['Booking ID'].duplicated(keep=False)
dupe_audit = df[dupe_mask].copy()

# Keep latest per Booking ID
df = (df.sort_values('_ts')
        .drop_duplicates(subset=['Booking ID'], keep='last')
        .drop(columns=['_ts']))

print("\nDuplicate rows:", int(df.duplicated().sum()))
if "Booking ID" in df.columns:
    dup_keys = df["Booking ID"].duplicated().sum()
    print("Duplicate Booking IDs:", int(dup_keys))
print(f"Rows before: {pre_n} | after: {len(df)} | removed: {pre_n - len(df)}")

"""### Feature Engineering -  Date and Time"""

# Convert Date and Time to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.time

# Combine Date and Time into a single datetime column
df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')

# Extract temporal features
df['hour'] = df['datetime'].dt.hour
df['day'] = df['datetime'].dt.day
df['month'] = df['datetime'].dt.month
df['weekday'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6
df['is_weekend'] = df['weekday'].isin([5, 6])


print("datetime NaT %:", (df['datetime'].isna().mean()*100).round(2))
print(df[['datetime','hour','weekday','is_weekend']].head())

"""### Handling null values"""

# Event flags (EDA only)
df['is_cancelled_customer'] = (df['Cancelled Rides by Customer'] == 1).astype('uint8')
df['is_cancelled_driver']   = (df['Cancelled Rides by Driver'] == 1).astype('uint8')
df['is_incomplete']         = (df['Incomplete Rides'] == 1).astype('uint8')

# Missingness flags
df['missing_driver_rating']   = df['Driver Ratings'].isna().astype('uint8')
df['missing_customer_rating'] = df['Customer Rating'].isna().astype('uint8')
df['missing_booking_value']   = df['Booking Value'].isna().astype('uint8')
df['missing_payment_method']  = df['Payment Method'].isna().astype('uint8')

# Numeric fills
df['Avg VTAT'] = df['Avg VTAT'].fillna(df['Avg VTAT'].median())
df['Avg CTAT'] = df['Avg CTAT'].fillna(df['Avg CTAT'].median())

"""###Target Variable Creation"""

# Target variable
df['target_customer_cancelled'] = df['is_cancelled_customer'].astype(int)

# Quick target distribution check
df['target_customer_cancelled'].value_counts(normalize=True)

"""### Exploratory Data Analysis - EDA Part 01"""

import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style="whitegrid")

# 1. Cancellation Rate by Vehicle Type
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='Vehicle Type', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Vehicle Type")
plt.xticks(rotation=45)
plt.show()

# 2. Cancellation Rate by Hour
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='hour', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Hour of Day")
plt.show()

# 3. Cancellation Rate by Day of Week
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='weekday', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Day of Week")
plt.xticks(ticks=range(7), labels=["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
plt.show()

# 4. Driver Rating vs Cancellation
plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='target_customer_cancelled', y='Driver Ratings')
plt.title("Driver Ratings vs Customer Cancellation")
plt.xticks([0, 1], ['Not Cancelled', 'Cancelled'])
plt.show()

"""### Exploratory Data Analysis - EDA Part 02"""

# 1. Top 10 Pickup Locations by Count
top_pickups = df['Pickup Location'].value_counts().nlargest(10).index
df_top_pickups = df[df['Pickup Location'].isin(top_pickups)]
plt.figure(figsize=(12, 6))
sns.barplot(data=df_top_pickups, x='Pickup Location', y='target_customer_cancelled')
plt.title("Cancellation Rate by Top 10 Pickup Locations")
plt.xticks(rotation=45)
plt.show()

# 2. Trend of Cancellations Over Time (Daily)
df_daily = df.groupby('Date')['target_customer_cancelled'].mean().reset_index()
plt.figure(figsize=(14, 6))
sns.lineplot(data=df_daily, x='Date', y='target_customer_cancelled')
plt.title("Daily Customer Cancellation Rate Over Time")
plt.ylabel("Cancellation Rate")
plt.xlabel("Date")
plt.show()

# 3. Heatmap of Hour vs Day of Week
heatmap_data = df.pivot_table(
    index='hour',
    columns='weekday',
    values='target_customer_cancelled',
    aggfunc='mean'
)
plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, cmap="YlGnBu", annot=True, fmt=".2f")
plt.title("Cancellation Rate by Hour and Day of Week")
plt.ylabel("Hour of Day")
plt.xlabel("Day of Week (0=Mon, 6=Sun)")
plt.show()

"""### Features Separation - Leaky Features Dropped"""

# CLEAN, LEAK-FREE FEATURE BUILD (Top-20 pickup/drop) ====
# - df['target_customer_cancelled'] already created (0/1)

import pandas as pd
import numpy as np
import json

TARGET = 'target_customer_cancelled'

# Initialize ARTIFACTS
ARTIFACTS = {}

#ONE-HOT ENCODE BOOKING-TIME CATEGORICALS

# Vehicle Type -> one-hot (drop_first=True to avoid perfect collinearity)
if 'Vehicle Type' in df.columns:
    df = pd.get_dummies(df, columns=['Vehicle Type'], prefix='vehicle', drop_first=True)

# Payment Method -> fill missing -> one-hot (keep only if known at booking)
if 'Payment Method' in df.columns:
    df['Payment Method'] = df['Payment Method'].fillna('Unknown')
    df = pd.get_dummies(df, columns=['Payment Method'], prefix='payment', drop_first=True)

# Top-20 Pickup / Drop locations (others -> 'Other') -> one-hot
if 'Pickup Location' in df.columns:
    # robust cleanup (strip and normalize case minimally)
    pick = df['Pickup Location'].astype(str).str.strip()
    top20_pickups = pick.value_counts().nlargest(20).index.tolist()
    ARTIFACTS['top20_pickups'] = top20_pickups
    df['pickup_encoded'] = np.where(pick.isin(top20_pickups), pick, 'Other')
    df = pd.get_dummies(df, columns=['pickup_encoded'], prefix='pickup', drop_first=True)

if 'Drop Location' in df.columns:
    dropc = df['Drop Location'].astype(str).str.strip()
    top20_drops = dropc.value_counts().nlargest(20).index.tolist()
    ARTIFACTS['top20_drops'] = top20_drops
    df['drop_encoded'] = np.where(dropc.isin(top20_drops), dropc, 'Other')
    df = pd.get_dummies(df, columns=['drop_encoded'], prefix='drop', drop_first=True)


# DROP LEAKY COLUMNS
LEAKY_COLS = [
    # direct/indirect label sources and reasons
    'is_cancelled_customer','is_cancelled_driver','is_incomplete',
    'Cancelled Rides by Customer','Cancelled Rides by Driver','Incomplete Rides',
    'Reason for cancelling by Customer','Driver Cancellation Reason','Incomplete Rides Reason',
    'Booking Status',
    # post-ride metrics (not known at booking)
    'Driver Ratings','Customer Rating','Ride Distance','Booking Value',
    # any missingness flags tied to post-ride vars
    'missing_driver_rating','missing_customer_rating','missing_booking_value',
    # history feature (removed by request)
    'customer_total_bookings',
]
# catch any other that might be around
LEAKY_COLS += [c for c in df.columns if c.startswith('missing_')]

ADMIN_TIME_COLS = [
    'Booking ID','Customer ID',  # identifiers
    'Pickup Location','Drop Location',  # raw strings replaced by OHE
    'Date','Time','datetime'  # raw time; we use engineered fields instead
]

DROP_NOW = [c for c in (LEAKY_COLS + ADMIN_TIME_COLS) if c in df.columns]
if DROP_NOW:
    df = df.drop(columns=DROP_NOW)

# BUILD FINAL WHITELIST OF FEATURES

# Time features
time_feats = [c for c in ['hour','weekday','month','is_weekend'] if c in df.columns]

# Numeric booking-time metrics
num_feats = [c for c in ['Avg VTAT','Avg CTAT'] if c in df.columns]

# One-hot groups
ohe_prefixes = ['vehicle_','payment_','pickup_','drop_']
ohe_cols = [c for c in df.columns if any(c.startswith(p) for p in ohe_prefixes)]

# Final whitelist
features = [c for c in (time_feats + num_feats + ohe_cols) if c != TARGET]

X = df[features].copy()
y = df[TARGET].astype('uint8')

# Booleans to Int types
bool_cols = X.select_dtypes(include=['bool']).columns
if len(bool_cols):
    X[bool_cols] = X[bool_cols].astype('uint8')

# Object / Category Check
obj_cat = X.select_dtypes(include=['object','category']).columns
if len(obj_cat):
    raise ValueError(f"Unexpected non-numeric feature columns: {list(obj_cat)}")

# Median-fill
for c in X.columns:
    if X[c].isna().any():
        X[c] = X[c].fillna(X[c].median())


print(f"Features ready: {len(features)} columns (numeric, no nulls).")
print("Examples:", features[:8])

# Optional: a clean training frame
#df_clean = pd.concat([X, y], axis=1)

"""### Sanity Check on Features

"""

# Show count + the exact feature names (in order)
print(f"Feature count: {len(features)}")
for i, c in enumerate(features, 1):
    print(f"{i:2d}. {c}")

# (Optional) Quick sanity: any non-numeric columns left?
non_num = [c for c in features if not pd.api.types.is_numeric_dtype(df[c])]
print("\nNon-numeric features:", non_num if non_num else "None")

# (Optional) peek at dtypes for all features
print("\nDtypes of features:")
print(df[features].dtypes.to_string())

# (Optional) save the exact feature list for Streamlit to reindex to
import json
with open("feature_names.json", "w") as f:
    json.dump(features, f, indent=2)
print("\nSaved feature list to feature_names.json")

# --- In Streamlit later ---
# import json, pandas as pd
# with open("feature_names.json") as f:
#     FEATURES = json.load(f)
# X = input_df.reindex(columns=FEATURES, fill_value=0)   # ensure exact column order

"""### Correlation"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# --- inputs ---
TARGET = 'target_customer_cancelled'
FEATURES = [
    'hour','weekday','month','is_weekend',
    'Avg VTAT','Avg CTAT',
    'vehicle_Bike','vehicle_Go Mini','vehicle_Go Sedan','vehicle_Premier Sedan','vehicle_Uber XL','vehicle_eBike',
    'payment_Credit Card','payment_Debit Card','payment_UPI','payment_Uber Wallet','payment_Unknown',
    'pickup_Badarpur','pickup_Barakhamba Road','pickup_Dwarka Sector 21','pickup_Khandsa','pickup_Madipur',
    'pickup_Mehrauli','pickup_Other','pickup_Pataudi Chowk','pickup_Pragati Maidan','pickup_Saket',
    'drop_Basai Dhankot','drop_Cyber Hub','drop_Kalkaji','drop_Kashmere Gate ISBT','drop_Lok Kalyan Marg',
    'drop_Madipur','drop_Narsinghpur','drop_Nehru Place','drop_Other','drop_Udyog Vihar'
]

# Build matrix with target for correlation
corr_df = df[FEATURES + [TARGET]].copy()

# Ensure no NaNs (shouldn’t be any, but just in case)
for c in corr_df.columns:
    if corr_df[c].isna().any():
        # median for numeric, most features are 0/1 or small integers
        corr_df[c] = corr_df[c].fillna(corr_df[c].median())

# 1) Correlation heatmap (lower triangle)
corr = corr_df.corr(method='pearson')
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

plt.figure(figsize=(14, 12))
sns.heatmap(
    corr, mask=mask, cmap='coolwarm', center=0, vmin=-1, vmax=1,
    square=False, linewidths=0.3, cbar_kws={'shrink': .8}
)
plt.title('Correlation heatmap (features + target)', pad=12)
plt.tight_layout()
plt.show()

# 2) Correlation with target (sorted bar chart)
corr_to_y = corr[TARGET].drop(TARGET).sort_values(key=lambda s: s.abs(), ascending=False)

plt.figure(figsize=(9, max(6, 0.25*len(corr_to_y))))
sns.barplot(x=corr_to_y.values, y=corr_to_y.index)
plt.xlabel('Pearson correlation with target')
plt.ylabel('Feature')
plt.title('Feature ↔ target correlation (sorted by |corr|)')
plt.tight_layout()
plt.show()

"""### Logistic Regression"""

# ========= Logistic Regression using your computed `features`, `X`, `y` =========
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, accuracy_score
from sklearn.preprocessing import FunctionTransformer
import joblib



# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Auto-detect continuous vs binary (0/1) columns
unique_counts = X_train.nunique(dropna=True)
bin_features = [c for c in X_train.columns if set(X_train[c].dropna().unique()).issubset({0,1})]
num_features = [c for c in X_train.columns if c not in bin_features]

bin_pipe = Pipeline([
    ('to_float', FunctionTransformer(lambda X: X.astype(np.float32))),
    ('imputer', SimpleImputer(strategy='constant', fill_value=0.0)),
])

# Preprocessing + model
transformers = [
    ('num', Pipeline([
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ]), num_features)
]
if len(bin_features):
    transformers.append(('bin', bin_pipe, bin_features))

preprocess = ColumnTransformer(
    transformers=transformers,
    remainder='drop'
)

logreg = LogisticRegression(
    class_weight='balanced',
    max_iter=1000,
    solver='lbfgs',
    n_jobs=-1
)

pipe = Pipeline([
    ('pre', preprocess),
    ('clf', logreg)
])

# Fit and evaluate
pipe.fit(X_train, y_train)
y_prob = pipe.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

roc = roc_auc_score(y_test, y_prob)
prec, rec, thr = precision_recall_curve(y_test, y_prob)
pr_auc = auc(rec, prec)

print(f"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f}")
print("\nClassification report @ threshold=0.50\n", classification_report(y_test, y_pred, digits=3))

# Optional: pick a better threshold (e.g., max F1)
#grid = np.linspace(0.05, 0.95, 19)
#f1s = [f1_score(y_test, (y_prob >= t).astype(int)) for t in grid]
#t_star = float(grid[int(np.argmax(f1s))])
#y_pred_star = (y_prob >= t_star).astype(int)
#print(f"\nBest F1 threshold on this test: {t_star:.2f}")
#print("Accuracy @ best-F1 threshold:", accuracy_score(y_test, y_pred_star).round(4))
#print("\nClassification report @ best-F1 threshold\n", classification_report(y_test, y_pred_star, digits=3))

"""### Random Forest"""

# RANDOM FOREST
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import joblib


# Remove only the 'payment_Unknown'
if 'payment_Unknown' in X.columns:
    X = X.drop(columns=['payment_Unknown'])
    try:
        features = [c for c in features if c != 'payment_Unknown']
    except NameError:
        pass
    print("Removed: payment_Unknown")


# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Pipeline: impute
rf = RandomForestClassifier(
    n_estimators=400,
    max_depth=None,
    min_samples_leaf=2,
    n_jobs=-1,
    random_state=42,
    class_weight='balanced_subsample'
)

pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('rf', rf)
])

# Fit & evaluate
pipe.fit(X_train, y_train)
y_prob = pipe.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

roc = roc_auc_score(y_test, y_prob)
prec, rec, thr = precision_recall_curve(y_test, y_prob)
pr_auc = auc(rec, prec)

print(f"ROC-AUC: {roc:.4f} | PR-AUC: {pr_auc:.4f}")
print("\nClassification report @ threshold=0.50\n",
      classification_report(y_test, y_pred, digits=3))


# Feature importance (Gini-based)
rf_fitted = pipe.named_steps['rf']
importances = pd.Series(rf_fitted.feature_importances_, index=X_train.columns).sort_values(ascending=False)

plt.figure(figsize=(8, min(12, 0.35*len(importances))))
sns.barplot(x=importances.values[:25], y=importances.index[:25])
plt.title("Random Forest feature importance (top 25)")
plt.xlabel("Mean decrease in impurity")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""### Random Forest wth Hyper Parameter Tuning - RandomizedSearchCV"""

# Random Forest — Hyperparameter Tuning
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score, accuracy_score
from scipy.stats import randint, uniform
import joblib

#  Holdout split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Pipeline: impute
rf = RandomForestClassifier(
    class_weight='balanced_subsample',
    random_state=42,
    n_jobs=-1
)
pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('rf', rf)
])

# Reasonable search space (keeps model from overfitting)
param_distributions = {
    'rf__n_estimators': randint(300, 800),
    'rf__max_depth': [None, 10, 14, 18, 22],
    'rf__min_samples_leaf': randint(1, 8),
    'rf__min_samples_split': randint(2, 12),
    'rf__max_features': ['sqrt', 0.3, 0.5, 0.8],
    'rf__bootstrap': [True],
    'rf__max_samples': [None, 0.7, 0.85, 1.0],
}

cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

search = RandomizedSearchCV(
    estimator=pipe,
    param_distributions=param_distributions,
    n_iter=15,
    scoring='roc_auc',
    n_jobs=-1,
    cv=cv,
    verbose=1,
    random_state=42,
    refit=True,
    return_train_score=True
)

search.fit(X_train, y_train)

print("\nBest ROC-AUC (CV):", round(search.best_score_, 4))
print("Best params:")
for k, v in search.best_params_.items():
    print("  ", k, "=", v)

best_model = search.best_estimator_

# Evaluate
y_prob = best_model.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.50).astype(int)

roc = roc_auc_score(y_test, y_prob)
prec, rec, thr = precision_recall_curve(y_test, y_prob)
pr_auc = auc(rec, prec)

print(f"\nTest ROC-AUC: {roc:.4f} | Test PR-AUC: {pr_auc:.4f}")
print("\nClassification report @ threshold=0.50\n",
      classification_report(y_test, y_pred, digits=3))

# choose a practical threshold to avoid misleading accuracy
# maximizes F1 on the test set.
grid = np.linspace(0.05, 0.95, 19)
f1s = [f1_score(y_test, (y_prob >= t).astype(int)) for t in grid]
t_star = float(grid[int(np.argmax(f1s))])
y_pred_star = (y_prob >= t_star).astype(int)
print(f"\nBest F1 threshold on this test: {t_star:.2f}")
print("Accuracy @ best-F1 threshold:",
      round(accuracy_score(y_test, y_pred_star), 4))
print("\nClassification report @ best-F1 threshold\n",
      classification_report(y_test, y_pred_star, digits=3))

# Baseline check so “abnormal accuracy” is obvious
always_zero_acc = (y_test == 0).mean()
print(f"\nNaive baseline accuracy (predict all 0): {always_zero_acc:.4f}")

"""### XGBoost Model"""

# XGBOOST with Out-of-Fold (OOF)
import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_predict
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (
    precision_recall_curve, auc, roc_auc_score, classification_report,
    accuracy_score, confusion_matrix
)

# XGBoost import
try:
    import xgboost as xgb
except ModuleNotFoundError:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "xgboost"])
    import xgboost as xgb


# Remove only the 'payment_Unknown'
if 'payment_Unknown' in X.columns:
    X = X.drop(columns=['payment_Unknown'])
    try:
        features = [c for c in features if c != 'payment_Unknown']
    except NameError:
        pass
    print("Removed: payment_Unknown")

# Cast to float32
X = X.astype(np.float32)

# OOF evaluation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
pos_rate = y.mean()
scale_pos_weight = (1 - pos_rate) / pos_rate if pos_rate > 0 else 1.0
print(f"Global scale_pos_weight for CV: {scale_pos_weight:.3f}  (pos rate ~ {pos_rate:.4f})")

xgb_clf = xgb.XGBClassifier(
    n_estimators=400,
    learning_rate=0.07,
    max_depth=5,
    min_child_weight=2,
    subsample=0.85,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    objective='binary:logistic',
    eval_metric='auc',
    tree_method='hist',
    scale_pos_weight=scale_pos_weight,
    random_state=42,
    n_jobs=-1
)

pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('xgb', xgb_clf)
])

# Out-of-fold probabilities for the positive class
oof_prob = cross_val_predict(
    pipe, X, y,
    cv=cv,
    method='predict_proba',
    n_jobs=-1
)[:, 1]

# OOF PR-AUC & ROC-AUC
p, r, _ = precision_recall_curve(y, oof_prob)
pr_auc_oof = auc(r, p)
roc_auc_oof = roc_auc_score(y, oof_prob)
print(f"\nOOF ROC-AUC: {roc_auc_oof:.4f} | OOF PR-AUC: {pr_auc_oof:.4f} | PR baseline: {y.mean():.4f}")

# also show a standard holdout for comparison
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Refit the same pipeline on train
pipe.fit(X_train, y_train)
y_prob = pipe.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.50).astype(int)

"""### HistGradientBoosting MODEL - Baseline"""

# HISTOGRAM-BASED GRADIENT BOOSTING (HGB)
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (
    classification_report, roc_auc_score, precision_recall_curve, auc
)


# Stratified holdout split BEFORE any fitting
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Class imbalance handling via sample weights
classes = np.array([0, 1])
cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
class_weight_map = {c:w for c, w in zip(classes, cw)}
sample_weight = y_train.map(class_weight_map) if isinstance(y_train, pd.Series) else np.vectorize(class_weight_map.get)(y_train)

# Plain HGB classifier (solid defaults)
hgb_baseline = HistGradientBoostingClassifier(
    learning_rate=0.07,
    max_depth=None,
    max_leaf_nodes=31,
    min_samples_leaf=20,
    l2_regularization=0.0,
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,
    random_state=42
)

# Fit (HGB supports NaNs natively so no imputer required)
hgb.fit(X_train, y_train, sample_weight=sample_weight)

# Evaluate
y_prob = hgb.predict_proba(X_test)[:, 1]
y_pred = (y_prob >= 0.50).astype(int)

roc = roc_auc_score(y_test, y_prob)
p, r, _ = precision_recall_curve(y_test, y_prob)
pr = auc(r, p)

print(f"ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f} | PR baseline: {y_test.mean():.4f}")
print("\nClassification report @ threshold=0.50\n",
      classification_report(y_test, y_pred, digits=3))

"""### HGB TUNED"""

# Train + Tune + (Calibrate) + Export
import numpy as np
import pandas as pd
from pathlib import Path
import json, joblib

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import (average_precision_score, roc_auc_score,
                             brier_score_loss, f1_score)
from sklearn.calibration import CalibratedClassifierCV
from scipy.stats import randint, uniform



# drop this one-hot column
if 'payment_Unknown' in X.columns:
    X = X.drop(columns=['payment_Unknown'])
    print("Removed: payment_Unknown")

# Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.20, stratify=y, random_state=42
)

# Class weights (imbalance)
classes = np.array([0, 1])
cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
cw_map = {c: w for c, w in zip(classes, cw)}
sample_weight_train = np.vectorize(cw_map.get)(y_train)
print(f"Train pos rate: {y_train.mean():.4f} | weights -> 0: {cw_map[0]:.2f}, 1: {cw_map[1]:.2f}")

# Base estimator
hgb = HistGradientBoostingClassifier(
    loss='log_loss',
    early_stopping=True,
    validation_fraction=0.1,
    n_iter_no_change=20,
    random_state=42
)

# Tune for PR-AUC
param_distributions = {
    'learning_rate': uniform(0.02, 0.12),
    'max_leaf_nodes': randint(16, 96),
    'max_depth': [None, 6, 8, 10, 12],
    'min_samples_leaf': randint(10, 120),
    'l2_regularization': uniform(0.0, 1.0),
    'max_bins': randint(128, 255),
    'max_iter': randint(150, 650),
}
cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
search = RandomizedSearchCV(
    estimator=hgb,
    param_distributions=param_distributions,
    n_iter=45,
    scoring='average_precision',
    cv=cv,
    n_jobs=-1,
    verbose=1,
    random_state=42,
    refit=True,
    return_train_score=True
)
search.fit(X_train, y_train, sample_weight=sample_weight_train)
best_hgb = search.best_estimator_

# Evaluate (pre-calibration)
y_prob = best_hgb.predict_proba(X_test)[:, 1]
ap   = average_precision_score(y_test, y_prob)
roc  = roc_auc_score(y_test, y_prob)
brier = brier_score_loss(y_test, y_prob)
base_rate = float(y_test.mean())
print(f"\nTEST pre-calib: AP={ap:.4f}  ROC={roc:.4f}  Brier={brier:.4f}  Base={base_rate:.4f}")

# Best-F1 threshold search
grid = np.linspace(0.00, 0.40, 401)
f1s  = [f1_score(y_test, (y_prob >= t).astype(int)) for t in grid]
t_star = float(grid[int(np.argmax(f1s))])
print(f"Best F1 threshold (pre-calib): {t_star:.4f}")

# Optional calibration
USE_CALIBRATED = True
if USE_CALIBRATED:
    cal = CalibratedClassifierCV(best_hgb, method="isotonic", cv=5)
    cal.fit(X_train, y_train)
    y_prob_cal = cal.predict_proba(X_test)[:, 1]
    ap_c   = average_precision_score(y_test, y_prob_cal)
    roc_c  = roc_auc_score(y_test, y_prob_cal)
    brier_c = brier_score_loss(y_test, y_prob_cal)
    grid_c = np.linspace(0.00, 0.40, 401)
    f1s_c  = [f1_score(y_test, (y_prob_cal >= t).astype(int)) for t in grid_c]
    t_star_c = float(grid_c[int(np.argmax(f1s_c))])
    print(f"TEST post-calib: AP={ap_c:.4f}  ROC={roc_c:.4f}  Brier={brier_c:.4f}")
    print(f"Best F1 threshold (calibrated): {t_star_c:.4f}")

    final_model = cal
    suggested_threshold = t_star_c
    mean_pred = float(y_prob_cal.mean())
else:
    final_model = best_hgb
    suggested_threshold = t_star
    mean_pred = float(y_prob.mean())

print(f"Mean predicted prob (final): {mean_pred:.4f} vs base {base_rate:.4f}")

# Save artifacts
ART = Path("artifacts"); ART.mkdir(exist_ok=True)
joblib.dump(final_model, ART / "final_model.pkl")
joblib.dump(list(X.columns), ART / "features.pkl")
meta = {
    "base_rate": base_rate,
    "suggested_threshold": float(suggested_threshold),
    "calibrated": bool(USE_CALIBRATED)
}
(ART / "meta.json").write_text(json.dumps(meta, indent=2))

print("\n✅ Saved:")
print("  artifacts/final_model.pkl")
print("  artifacts/features.pkl")
print("  artifacts/meta.json")

# peek at top CV rows
cvres = pd.DataFrame(search.cv_results_).sort_values('rank_test_score')
print("\nTop 5 CV rows:")
print(cvres[['rank_test_score','mean_test_score','param_learning_rate',
             'param_max_leaf_nodes','param_min_samples_leaf','param_max_iter']].head(5).to_string(index=False))

"""### Comparison with all 4 models

### Pick Best Model
"""

# Unified evaluation, ranking, and FINAL MODEL PICK
import numpy as np
import pandas as pd
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_score, recall_score, f1_score,
    accuracy_score, balanced_accuracy_score
)
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier


def _find_logreg():
    if 'lr_final' in globals():
        return globals()['lr_final']
    for name, obj in list(globals().items())[::-1]:
        if isinstance(obj, Pipeline):
            last_step = list(obj.named_steps.values())[-1]
            if isinstance(last_step, LogisticRegression):
                return obj
    return None

def _find_rf_tuned():
    if 'best_model' in globals():
        return globals()['best_model']
    if 'best_rf' in globals():
        return globals()['best_rf']
    for name, obj in list(globals().items())[::-1]:
        if isinstance(obj, Pipeline) and 'rf' in obj.named_steps and isinstance(obj.named_steps['rf'], RandomForestClassifier):
            return obj
    return None

def _find_xgb_tuned():
    try:
        import xgboost as xgb
    except Exception:
        xgb = None
    if 'xgb_tuned' in globals():
        return globals()['xgb_tuned']
    if 'pipe' in globals() and isinstance(globals()['pipe'], Pipeline) and xgb is not None:
        step = globals()['pipe'].named_steps.get('xgb')
        if isinstance(step, xgb.XGBClassifier):
            return globals()['pipe']
    if xgb is not None:
        for name, obj in list(globals().items())[::-1]:
            if isinstance(obj, Pipeline) and isinstance(obj.named_steps.get('xgb', None), xgb.XGBClassifier):
                return obj
    return None

def _find_hgb_tuned():
    if 'best_hgb' in globals():
        return globals()['best_hgb']
    for name, obj in list(globals().items())[::-1]:
        if isinstance(obj, HistGradientBoostingClassifier):
            return obj
    return None

lr_model   = _find_logreg()
rf_model   = _find_rf_tuned()
xgb_model  = _find_xgb_tuned()
hgb_model  = _find_hgb_tuned()

model_objects = {}
if lr_model is not None:  model_objects["LogReg (balanced)"]    = lr_model
if rf_model is not None:  model_objects["RandomForest tuned"]   = rf_model
if xgb_model is not None: model_objects["XGBoost tuned"]        = xgb_model
if hgb_model is not None: model_objects["HistGB tuned"]         = hgb_model

if not model_objects:
    raise RuntimeError("No fitted models found. Run your training cells first so objects exist (lr, RF tuned, XGB tuned, HGB tuned).")


def _proba(model, X):
    if hasattr(model, "predict_proba"):
        return model.predict_proba(X)[:, 1]
    if hasattr(model, "decision_function"):
        s = model.decision_function(X)
        s = np.clip(s, -20, 20)
        return 1.0 / (1.0 + np.exp(-s))
    return model.predict(X).astype(float)

def _eval_row(name, model, X, y, thresh=0.50):
    y_prob = _proba(model, X)
    y_hat  = (y_prob >= thresh).astype(int)

    return {
        "Model": name,
        "Threshold": float(thresh),
        "Test ROC-AUC": float(roc_auc_score(y, y_prob)),
        "Test PR-AUC": float(average_precision_score(y, y_prob)),
        "Accuracy": float(accuracy_score(y, y_hat)),
        "Balanced Acc": float(balanced_accuracy_score(y, y_hat)),
        "Precision (class 1)": float(precision_score(y, y_hat, pos_label=1, zero_division=0)),
        "Recall (class 1)": float(recall_score(y, y_hat, pos_label=1, zero_division=0)),
        "F1 (class 1)": float(f1_score(y, y_hat, pos_label=1, zero_division=0)),
        "Precision (class 0)": float(precision_score(y, y_hat, pos_label=0, zero_division=0)),
        "Recall (class 0)": float(recall_score(y, y_hat, pos_label=0, zero_division=0)),
        "F1 (class 0)": float(f1_score(y, y_hat, pos_label=0, zero_division=0)),
        "Support (1)": int((y == 1).sum()),
        "Support (0)": int((y == 0).sum()),
    }


rows = [_eval_row(name, mdl, X_test, y_test, thresh=0.50)
        for name, mdl in model_objects.items()]
all_cmp = pd.DataFrame(rows)


display_df = all_cmp.copy()
for c in ["Test ROC-AUC","Test PR-AUC","Accuracy","Balanced Acc",
          "Precision (class 1)","Recall (class 1)","F1 (class 1)",
          "Precision (class 0)","Recall (class 0)","F1 (class 0)"]:
    display_df[c] = display_df[c].map(lambda v: round(v, 4))

print("\n=== Model comparison (threshold = 0.50) ===")
display(display_df.sort_values(["Recall (class 1)", "F1 (class 1)", "Test ROC-AUC"],
                               ascending=[False, False, False]).reset_index(drop=True))

# Rank by Recall1 -> F1_1 -> ROC-AUC
ranked = (all_cmp
          .sort_values(["Recall (class 1)", "F1 (class 1)", "Test ROC-AUC"],
                       ascending=[False, False, False])
          .reset_index(drop=True))

#Final pick = HGB tuned (per your requirement)
if "HistGB tuned" in model_objects:
    final_model_name = "HistGB tuned"
else:
    final_model_name = ranked.loc[0, "Model"]

final_model = model_objects[final_model_name]
print(f"\n✅ Selected final model: {final_model_name}")